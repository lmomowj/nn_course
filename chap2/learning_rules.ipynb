{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7b88022",
   "metadata": {},
   "source": [
    "# 神经网络学习规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45c94dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c33dcb",
   "metadata": {},
   "source": [
    "###  定义激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383faeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear 函数\n",
    "def linear(x,w):\n",
    "    # x,w 行向量\n",
    "    return np.dot(w,x.T)    \n",
    "\n",
    "# Logistic 函数\n",
    "def logistic(z):\n",
    "    return 1./(1+np.exp(-z))\n",
    "def logistic_gradient(z):\n",
    "    f = logistic(z)\n",
    "    return f*(1-f)    \n",
    "\n",
    "# Tanh 函数\n",
    "def tanh(z):\n",
    "    return (np.exp(z)-np.exp(-z))/(np.exp(z) + np.exp(-z))\n",
    "def tanh_gradient(z):\n",
    "    f = tanh(z)\n",
    "    return (1-f**2)\n",
    "\n",
    "# 双极性S型曲线\n",
    "def sigmoid(z):\n",
    "    return (1-np.exp(-z))/(1+np.exp(-z))\n",
    "def sigmoid_gradient(z):\n",
    "    f = sigmoid(z)\n",
    "    return (1-f**2)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38b980f",
   "metadata": {},
   "source": [
    "###  定义学习规则"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b99a5f",
   "metadata": {},
   "source": [
    "##  Hebb 学习规则"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe54c90",
   "metadata": {},
   "source": [
    "&nbsp;学习信号: &nbsp; $r = f(net) = f(W X^T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af968d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hebb学习规则\n",
    "def hebb(x,w,act='sgn'):\n",
    "    # x,w 行向量\n",
    "    # hebb规则：r(x)    \n",
    "    net = linear(x,w)\n",
    "    print('净输入: net = w * x.T = %.3f'%net)    \n",
    "    if act == 'sgn':\n",
    "        out = np.sign(net)\n",
    "    elif act == 'sigmoid_d':\n",
    "        out = sigmoid(net)\n",
    "    elif act == 'tanh':\n",
    "        out = np.tanh(net)\n",
    "    print('实际输出: o = f(net) = %.3f'% out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3163ff25",
   "metadata": {},
   "source": [
    "##  Perceptron (感知器) 学习规则"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f79e2",
   "metadata": {},
   "source": [
    "&nbsp;学习信号: &nbsp; $r = d - o = d - f(net) = d - \\mathrm{sgn}(W X^T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92a41f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron 学习规则\n",
    "def perceptron(x,y,w):\n",
    "    # x,w 行向量\n",
    "    # perceptron 学习规则：r(x)\n",
    "    net = linear(x,w)\n",
    "    print('净输入: net = w * x.T =%.3f'% net)\n",
    "    out = np.sign(net)\n",
    "    print('实际输出: o = f(net) = %.3f'% out)\n",
    "    return y -out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99269b0",
   "metadata": {},
   "source": [
    "##  $\\delta$ 学习规则"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e21d3b",
   "metadata": {},
   "source": [
    "&nbsp;学习信号: &nbsp; $r = [d - f(net)]f'(net) = [d - f(W X^T)]f'(W X^T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a97400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta学习规则\n",
    "def delta(x,y,w,act='logistic'):\n",
    "    # x,w 行向量\n",
    "    # Delta学习规则：r(x)    \n",
    "    net = linear(x,w)\n",
    "    print('净输入: net = w * x.T = %.3f'%net)  \n",
    "    # 激活函数\n",
    "    if act == 'logistic':\n",
    "        out = logistic(net)\n",
    "        df = logistic_gradient(net)\n",
    "    elif act == 'tanh':\n",
    "        out = tanh(net)\n",
    "        df = tanh_gradient(net)\n",
    "    elif act == 'sigmoid_d':\n",
    "        out = sigmoid(net)\n",
    "        df = sigmoid_gradient(net)\n",
    "    print('实际输出: o = f(net) = %.3f'% out)\n",
    "    print(\"导数：f'(net) = %.3f\"% df)\n",
    "    return (y-out)*df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e61d01",
   "metadata": {},
   "source": [
    "##  Widrow-Hoff (最小均方)学习规则"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac0ee0c",
   "metadata": {},
   "source": [
    "&nbsp;学习信号: &nbsp; $r = d - net = d - W X^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6270f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widrow-hoff (LMS)学习规则\n",
    "def LMS(x,y,w):\n",
    "    # x,w 行向量\n",
    "    # Widrow-hoff (LMS)学习规则：r(x)    \n",
    "    out = linear(x,w)\n",
    "    print('净输入: net = w * x.T = %.3f'%out)\n",
    "    print('实际输出: o = net = %.3f'% out)\n",
    "    return y-out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395dfbfc",
   "metadata": {},
   "source": [
    "##  Correlation (相关)学习规则"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b33950",
   "metadata": {},
   "source": [
    "&nbsp;学习信号: &nbsp; $r = d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b685a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation学习规则\n",
    "def corre(x,y,w):\n",
    "    # x,w 行向量\n",
    "    # Correlation学习规则：r = y \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9192f234",
   "metadata": {},
   "source": [
    "###  &nbsp; &nbsp; 权值更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac1e8d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 权值更新\n",
    "def update(epoch,X,y,w,rule,act,lr): \n",
    "    i = epoch % X.shape[0]\n",
    "    print(\"\\nEpoch: %d\"% epoch)\n",
    "    print('当前输入的训练数据: ',X[i],y[i])    \n",
    "    if rule == 'hebb':  \n",
    "        rs = hebb(X[i],w,act)    # 学习信号\n",
    "    elif rule == 'perceptron':\n",
    "        rs = perceptron(X[i],y[i],w)    # 学习信号\n",
    "    elif rule == 'delta':\n",
    "        rs = delta(X[i],y[i],w,act)\n",
    "    elif rule == 'LMS':\n",
    "        rs = LMS(X[i],y[i],w)\n",
    "    print('学习信号：r(w,x,d) = %.3f'% rs)\n",
    "    dw = lr * rs * X[i]   # 权值调整量\n",
    "    w = w + dw  \n",
    "    print('更新的权值向量: ',w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb1acbc",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 运行权值学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46b05f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行 权值学习\n",
    "def runner(X,y=None,rule='hebb',act='sgn',lr=0.1,epochs=10,init_w=None,init_T=None):\n",
    "    # 样本\n",
    "    if y is None:\n",
    "        y = X.shape[0]*[None]\n",
    "    # 初始化权向量\n",
    "    if init_w is None:\n",
    "        w_ = np.zeros(X_.shape[1])  \n",
    "    else:   \n",
    "        w_ = np.array(init_w)        \n",
    "    print('初始权值向量: ',w_)\n",
    "    \n",
    "    # 更新权值向量 （训练）\n",
    "    for epoch in range(epochs):   \n",
    "        w = update(epoch,X,y,w_,rule,act,lr)        \n",
    "        if (w == w_).all():\n",
    "            print('\\n学习结束')\n",
    "            print(f'算法迭代次数: {epoch}')\n",
    "            break\n",
    "        else:\n",
    "            w_ = w        \n",
    "    return w_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e87852b",
   "metadata": {},
   "source": [
    "&nbsp; $\\cdot $ &nbsp; Hebbian学习规则的应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b69bac71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "学习规则：hebb\n",
      "学习率：1\n",
      "初始权值向量:  [ 1.  -1.   0.   0.5]\n",
      "\n",
      "Epoch: 0\n",
      "当前输入的训练数据:  [ 1.  -2.   1.5  0. ] None\n",
      "净输入: net = w * x.T = 3.000\n",
      "实际输出: o = f(net) = 1.000\n",
      "学习信号：r(w,x,d) = 1.000\n",
      "更新的权值向量:  [ 2.  -3.   1.5  0.5]\n",
      "\n",
      "Epoch: 1\n",
      "当前输入的训练数据:  [ 1.  -0.5 -2.  -1.5] None\n",
      "净输入: net = w * x.T = -0.250\n",
      "实际输出: o = f(net) = -1.000\n",
      "学习信号：r(w,x,d) = -1.000\n",
      "更新的权值向量:  [ 1.  -2.5  3.5  2. ]\n",
      "\n",
      "Epoch: 2\n",
      "当前输入的训练数据:  [ 0.   1.  -1.   1.5] None\n",
      "净输入: net = w * x.T = -3.000\n",
      "实际输出: o = f(net) = -1.000\n",
      "学习信号：r(w,x,d) = -1.000\n",
      "更新的权值向量:  [ 1.  -3.5  4.5  0.5]\n"
     ]
    }
   ],
   "source": [
    "# Hebbian学习规则\n",
    "# P33 例2.1 样本\n",
    "X = np.array([[1,-2,1.5,0],[1,-0.5,-2,-1.5],[0,1,-1,1.5]])\n",
    "lr = 1    # 学习率\n",
    "rule =  'hebb' # 学习规则\n",
    "act =  'sgn' # 'sigmoid_d' #   # 激活函数\n",
    "epochs = 3    # 训练次数\n",
    "init_w = np.array([1,-1,0,0.5])  # 初始化权向量  \n",
    "\n",
    "print(f'\\n学习规则：{rule}')\n",
    "print(f'学习率：{lr}')\n",
    "\n",
    "final_w = runner(X,y=None,rule=rule,act=act,lr=lr,epochs=epochs,init_w=init_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d611fc95",
   "metadata": {},
   "source": [
    "&nbsp; $\\cdot $ &nbsp;$\\delta$ 学习规则的应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe671003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "学习规则：delta\n",
      "学习率：0.1\n",
      "初始权值向量:  [ 0.5  1.  -1.   0. ]\n",
      "\n",
      "Epoch: 0\n",
      "当前输入的训练数据:  [-1.  1. -2.  0.] -1\n",
      "净输入: net = w * x.T = 2.500\n",
      "实际输出: o = f(net) = 0.848\n",
      "导数：f'(net) = 0.140\n",
      "学习信号：r(w,x,d) = -0.259\n",
      "更新的权值向量:  [ 0.526  0.974 -0.948  0.   ]\n",
      "\n",
      "Epoch: 1\n",
      "当前输入的训练数据:  [-1.   0.   1.5 -0.5] -1\n",
      "净输入: net = w * x.T = -1.948\n",
      "实际输出: o = f(net) = -0.750\n",
      "导数：f'(net) = 0.218\n",
      "学习信号：r(w,x,d) = -0.054\n",
      "更新的权值向量:  [ 0.531  0.974 -0.956  0.003]\n",
      "\n",
      "Epoch: 2\n",
      "当前输入的训练数据:  [-1.  -1.   1.   0.5] 1\n",
      "净输入: net = w * x.T = -2.460\n",
      "实际输出: o = f(net) = -0.843\n",
      "导数：f'(net) = 0.145\n",
      "学习信号：r(w,x,d) = 0.267\n",
      "更新的权值向量:  [ 0.505  0.947 -0.93   0.016]\n"
     ]
    }
   ],
   "source": [
    "# Delta 学习规则\n",
    "# P37 例2.2  Delta学习规则\n",
    "# 样本\n",
    "X = np.array([[-1,1,-2,0],[-1,0,1.5,-0.5],[-1,-1,1,0.5]])\n",
    "#X = np.array([[-1,1,-2,0],[-1,0,1.5,-0.5],[-1,1,0.5,-1]])\n",
    "# 期望输出 d = y\n",
    "y = np.array([-1,-1,1]) \n",
    "lr = 0.1    # 学习率\n",
    "rule =  'delta' # 学习规则\n",
    "act = 'sigmoid_d' #    激活函数\n",
    "epochs = 3    # 训练次数\n",
    "init_w = np.array([0.5,1,-1,0]) # 初始化权向量  \n",
    "\n",
    "print(f'\\n学习规则：{rule}')\n",
    "print(f'学习率：{lr}')\n",
    "\n",
    "final_w = runner(X,y=y,rule=rule,act=act,lr=lr,epochs=epochs,init_w=init_w)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "原始单元格格式",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
